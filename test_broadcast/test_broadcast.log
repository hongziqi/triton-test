============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.3.2, pluggy-1.6.0 -- /home/coder/miniconda/envs/origin-triton/bin/python3.10
cachedir: .pytest_cache
rootdir: /home/coder/workspace/triton-test
plugins: anyio-4.9.0, xdist-3.6.1
collecting ... collected 12 items

language/test_core.py::test_broadcast[int8] Dumping intermediate results to /home/coder/.triton/dump/-1kF2jjcOpZyJE85Cle2zsJxWBCnovHL2zDoqiD0pmI
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/HjM41PmXCEDZIQYkZCcBTh3qxcpOdm0Rb2TMm4f-WOI
PASSED
language/test_core.py::test_broadcast[int16] Dumping intermediate results to /home/coder/.triton/dump/b6aPufrQuyXxAZRhItUVzkRYIRKifprbktoKrHBtF3A
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/VXZYsHVBHCjsuzFsW7EhjT_PZLyHtIJ-j74xoLI1Wns
PASSED
language/test_core.py::test_broadcast[int32] Dumping intermediate results to /home/coder/.triton/dump/v1DdzmfI8cRgMEwJPAST9_zPwE-011CZlPkLG-uf5CQ
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/OnW8GPm4y_XufPcs0oQrLP96Krj3vfSWHobvAFxBIMw
PASSED
language/test_core.py::test_broadcast[int64] Dumping intermediate results to /home/coder/.triton/dump/CdQqMdwnXwIx8kL5L7KOApVzuvvesfrBr8xK6tPd4SA
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/DWct2eiYyVawze38J07lCwMv954znaEQJwZ0Pc4QMC4
PASSED
language/test_core.py::test_broadcast[uint8] Dumping intermediate results to /home/coder/.triton/dump/Om85nkZWgEk7g4TtygEciOJUI0ny3v1gyzTqYWC_b1M
FAILED
language/test_core.py::test_broadcast[uint16] Dumping intermediate results to /home/coder/.triton/dump/p0_7sheAnBXJEHUWZmqgQIAnbl4EIHQGKVXqImIcUO4
FAILED
language/test_core.py::test_broadcast[uint32] Dumping intermediate results to /home/coder/.triton/dump/_yrYUzZxhvLuOceoUQH9e0JxvzDGpO_wEjr3-G3I25I
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/SWpxhmmXKCithqMcSanl6ffvOG4ALttV5X2On0yqa0k
PASSED
language/test_core.py::test_broadcast[uint64] Dumping intermediate results to /home/coder/.triton/dump/pn64vTFKWdic0iqY8XhxAtXkTuX_KsJFQvzhw_tdGlU
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/9u8KKykXan4AK49BU4kfxrli50nJIYNdmol8-z8n3K4
PASSED
language/test_core.py::test_broadcast[float16] Dumping intermediate results to /home/coder/.triton/dump/MZnaY67bZLqcpw2SHD9W2oSOwjZ9lORjinzyUiPtE3A
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/hFkeGtljuXCWRew5KDq3EELB1Sw1CgogUloqS6JP1Xo
PASSED
language/test_core.py::test_broadcast[float32] Dumping intermediate results to /home/coder/.triton/dump/MZ0X6oyQsVsSC8_9hzbjiCbl9ki9VDyZQCLb6qJb9Gg
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/mowkVqjXJbVkXYnx6gRbFtLsZvbW05UiDZmPnLIMvwo
PASSED
language/test_core.py::test_broadcast[float64] FAILED
language/test_core.py::test_broadcast[bfloat16] Dumping intermediate results to /home/coder/.triton/dump/KUiub42vo-qNNF_Fn-FUhsSiS1ONOAKXji9L4kQLBgM
Dumping launcher_cxx11abi1.cxx to /home/coder/.triton/dump/X9gF92VdyBv_ImwssKVKAKDBgNVyyetkU52ndCfiYEI
PASSED

=================================== FAILURES ===================================
____________________________ test_broadcast[uint8] _____________________________

dtype = 'uint8', device = 'npu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("dtype", dtypes_with_bfloat16)
    def test_broadcast(dtype, device):
        # check_type_supported(dtype, device)
    
        @triton.jit
        def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):
            offset1 = tl.arange(0, M)
            offset2 = tl.arange(0, N)
            x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])
            y = tl.load(y_ptr + offset2)
            _, y_broadcasted = tl.broadcast(x, y)
            tl.store(y_broadcasted_ptr + N * offset1[:, None] + offset2[None, :], y_broadcasted)
    
        M = 32
        N = 64
        rs = RandomState(17)
        x = numpy_random((M, N), dtype_str=dtype, rs=rs)
        y = numpy_random(N, dtype_str=dtype, rs=rs)
        _, y_broadcasted_np = np.broadcast_arrays(x, y)
    
        x_tri = to_triton(x, device=device, dst_type=dtype)
        y_tri = to_triton(y, device=device, dst_type=dtype)
        y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)
    
>       broadcast_kernel[(1, )](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)

language/test_core.py:650: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:331: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:670: in run
    kernel._init_handles()
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/compiler/compiler.py:401: in _init_handles
    self.run = driver.active.launcher_cls(self.src, self.metadata)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:79: in __init__
    wrapper_src = generate_npu_wrapper_src(constants, signature, \
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:469: in generate_npu_wrapper_src
    {LINE_CHANGE_CHAR.join(
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:470: in <genexpr>
    f'dataTypes[{i}] = {convert_sigtype_to_int(ty[1:])};'
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

sigty = 'u8'

    def convert_sigtype_to_int(sigty: str):
        MAP_SIGTYPE_TO_INT = {
            # Boolean
            "i1": 12,  # BOOL
            # Integer types
            "i8": 2,  # INT8
            "i16": 6,  # INT16
            "i32": 3,  # INT32
            "i64": 9,  # INT64
            # Unsigned integer types
            "u32": 8,  # UINT32
            "u64": 10,  # UINT64
            # Floating point types
            "fp16": 1,  # FLOAT16
            "bf16": 27,  # DT_BF16
            "fp32": 0,  # FLOAT
            "fp64": 11,  # DOUBLE
        }
        if sigty not in MAP_SIGTYPE_TO_INT:
>           raise ValueError(f"Unsupported data type: {sigty}")
E           ValueError: Unsupported data type: u8

../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/utils.py:287: ValueError
____________________________ test_broadcast[uint16] ____________________________

dtype = 'uint16', device = 'npu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("dtype", dtypes_with_bfloat16)
    def test_broadcast(dtype, device):
        # check_type_supported(dtype, device)
    
        @triton.jit
        def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):
            offset1 = tl.arange(0, M)
            offset2 = tl.arange(0, N)
            x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])
            y = tl.load(y_ptr + offset2)
            _, y_broadcasted = tl.broadcast(x, y)
            tl.store(y_broadcasted_ptr + N * offset1[:, None] + offset2[None, :], y_broadcasted)
    
        M = 32
        N = 64
        rs = RandomState(17)
        x = numpy_random((M, N), dtype_str=dtype, rs=rs)
        y = numpy_random(N, dtype_str=dtype, rs=rs)
        _, y_broadcasted_np = np.broadcast_arrays(x, y)
    
        x_tri = to_triton(x, device=device, dst_type=dtype)
        y_tri = to_triton(y, device=device, dst_type=dtype)
        y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)
    
>       broadcast_kernel[(1, )](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)

language/test_core.py:650: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:331: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:670: in run
    kernel._init_handles()
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/compiler/compiler.py:401: in _init_handles
    self.run = driver.active.launcher_cls(self.src, self.metadata)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:79: in __init__
    wrapper_src = generate_npu_wrapper_src(constants, signature, \
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:469: in generate_npu_wrapper_src
    {LINE_CHANGE_CHAR.join(
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/driver.py:470: in <genexpr>
    f'dataTypes[{i}] = {convert_sigtype_to_int(ty[1:])};'
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

sigty = 'u16'

    def convert_sigtype_to_int(sigty: str):
        MAP_SIGTYPE_TO_INT = {
            # Boolean
            "i1": 12,  # BOOL
            # Integer types
            "i8": 2,  # INT8
            "i16": 6,  # INT16
            "i32": 3,  # INT32
            "i64": 9,  # INT64
            # Unsigned integer types
            "u32": 8,  # UINT32
            "u64": 10,  # UINT64
            # Floating point types
            "fp16": 1,  # FLOAT16
            "bf16": 27,  # DT_BF16
            "fp32": 0,  # FLOAT
            "fp64": 11,  # DOUBLE
        }
        if sigty not in MAP_SIGTYPE_TO_INT:
>           raise ValueError(f"Unsupported data type: {sigty}")
E           ValueError: Unsupported data type: u16

../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/backends/ascend/utils.py:287: ValueError
___________________________ test_broadcast[float64] ____________________________

args = (<triton.language.core.tensor object at 0xfffdbdfff3a0>,)
kwargs = {'_builder': <triton._C.libtriton.ir.builder object at 0xfffdbe040130>}

    @wraps(fn)
    def wrapper(*args, **kwargs):
        if "_builder" not in kwargs or kwargs["_builder"] is None:
            print(kwargs)
            raise ValueError("Did you forget to add @triton.jit ? "
                             "(`_builder` argument must be provided outside of JIT functions.)")
>       return fn(*args, **kwargs)

../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/language/core.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/language/core.py:1635: in load
    return semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/language/semantic.py:1141: in load
    return _load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/triton_patch/language/semantic.py:416: in _load_legacy
    other = cast(other, elt_ty, builder)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <triton.language.core.tensor object at 0xfffdbdffe770>
dst_ty = <[32, 64], fp64>
builder = <triton._C.libtriton.ir.builder object at 0xfffdbe040130>
fp_downcast_rounding = None

    def cast(input: tl.tensor, dst_ty: tl.dtype, builder: ir.builder,
             fp_downcast_rounding: Optional[str] = None) -> tl.tensor:
        src_ty = input.type
        if isinstance(dst_ty, tl.constexpr):
            dst_ty = dst_ty.value
        if isinstance(fp_downcast_rounding, tl.constexpr):
            fp_downcast_rounding = fp_downcast_rounding.value
        if src_ty.is_block():
            dst_ty = tl.block_type(dst_ty.scalar, input.type.get_block_shapes())
        if src_ty == dst_ty:
            return input
    
        src_sca_ty = src_ty.scalar
        dst_sca_ty = dst_ty.scalar
    
        # For fp downcasting default rounding mode should be RTNE, for all other conversions it should
        # not be set
        fp_downcast_rounding = _str_to_rounding_mode(fp_downcast_rounding)
        use_custom_rounding = False
        if dst_sca_ty.is_floating() and src_sca_ty.is_floating(
        ) and dst_sca_ty.primitive_bitwidth < src_sca_ty.primitive_bitwidth:
            if fp_downcast_rounding is None: fp_downcast_rounding = ir.ROUNDING_MODE.RTNE
            elif fp_downcast_rounding != ir.ROUNDING_MODE.RTNE: use_custom_rounding = True
        else:
            if fp_downcast_rounding is not None:
                raise ValueError("fp_downcast_rounding should be set only for truncating fp conversions. "
                                 "Source scalar type is " + str(src_sca_ty) + " and destination type is " + str(dst_sca_ty))
    
        if (src_sca_ty.is_fp8() or dst_sca_ty.is_fp8()) or (src_sca_ty.is_fp64() or dst_sca_ty.is_fp64()):
>           raise ValueError("[fp8, fp64] is unsupported on Ascend for now."
                             "Source scalar type is " + str(src_sca_ty) + " and destination type is " + str(dst_sca_ty))
E           ValueError: [fp8, fp64] is unsupported on Ascend for now.Source scalar type is int32 and destination type is fp64

../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/triton_patch/language/semantic.py:57: ValueError

The above exception was the direct cause of the following exception:

dtype = 'float64', device = 'npu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("dtype", dtypes_with_bfloat16)
    def test_broadcast(dtype, device):
        # check_type_supported(dtype, device)
    
        @triton.jit
        def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):
            offset1 = tl.arange(0, M)
            offset2 = tl.arange(0, N)
            x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])
            y = tl.load(y_ptr + offset2)
            _, y_broadcasted = tl.broadcast(x, y)
            tl.store(y_broadcasted_ptr + N * offset1[:, None] + offset2[None, :], y_broadcasted)
    
        M = 32
        N = 64
        rs = RandomState(17)
        x = numpy_random((M, N), dtype_str=dtype, rs=rs)
        y = numpy_random(N, dtype_str=dtype, rs=rs)
        _, y_broadcasted_np = np.broadcast_arrays(x, y)
    
        x_tri = to_triton(x, device=device, dst_type=dtype)
        y_tri = to_triton(y, device=device, dst_type=dtype)
        y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)
    
>       broadcast_kernel[(1, )](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)

language/test_core.py:650: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:331: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/runtime/jit.py:635: in run
    kernel = self.compile(
../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/compiler/compiler.py:281: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0xfffdbdfffa00>
options = NPUOptions(debug=True, sanitize_overflow=True, llvm_version=15, kernel_name='triton_', cluster_dims=(1, 1, 1), num_war...input_precisions=('ieee', 'hf32'), max_num_imprecise_acc_default=None, extern_libs=None, multibuffer=True, stream=None)
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0xfffdbdefd5a0>}
module_map = {}
context = <triton._C.libtriton.ir.context object at 0xfffdbf893570>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 4:8:
E       def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):
E           offset1 = tl.arange(0, M)
E           offset2 = tl.arange(0, N)
E           x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])
E               ^
E       ValueError('[fp8, fp64] is unsupported on Ascend for now.Source scalar type is int32 and destination type is fp64')

../../miniconda/envs/origin-triton/lib/python3.10/site-packages/triton/compiler/compiler.py:102: CompilationError
=========================== short test summary info ============================
FAILED language/test_core.py::test_broadcast[uint8] - ValueError: Unsupported...
FAILED language/test_core.py::test_broadcast[uint16] - ValueError: Unsupporte...
FAILED language/test_core.py::test_broadcast[float64] - triton.compiler.error...
=================== 3 failed, 9 passed in 174.70s (0:02:54) ====================
